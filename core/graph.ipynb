{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-17T08:44:06.621713Z",
     "start_time": "2025-05-17T08:44:05.894449Z"
    }
   },
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from core.graph import builder\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# Create a memory-based checkpointer and compile the graph\n",
    "# This enables state persistence and tracking throughout the workflow execution\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T08:44:32.201503Z",
     "start_time": "2025-05-17T08:44:06.941807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import uuid\n",
    "from IPython.display import Markdown\n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\": str(uuid.uuid4()),\n",
    "                           \"planner_provider\": \"openai\",\n",
    "                           \"planner_model\": \"o3-mini\",\n",
    "                           \"writer_provider\": \"openai\",\n",
    "                           \"writer_model\": \"gpt-4.1-nano\",\n",
    "                           \"file_type\": \"CSV\",\n",
    "                           \"index4file\": \"true\",\n",
    "                           \"Breadth\": 2\n",
    "                           }}\n",
    "\n",
    "# Define research topic about Model Context Protocol\n",
    "file_path = r\"C:\\Users\\kaiyi\\Desktop\\github\\automatic-tool-generation\\dataset\\synthetic_server_metrics.csv\"\n",
    "event = {\"orig_file_path\": file_path,}\n",
    "# Run the graph workflow until first interruption (waiting for user feedback)\n",
    "s_lists = []\n",
    "# for s in graph.stream(event, thread, stream_mode=\"updates\", subgraphs=True):\n",
    "#     print(\"================================\")\n",
    "#     print(s)\n",
    "#     s_lists.append(s)\n",
    "async for s in graph.astream(event, thread, stream_mode=\"updates\"):\n",
    "    print(\"================================\")\n",
    "    print(s)\n",
    "    s_lists.append(s)\n",
    "print(\"======================================\")\n",
    "\n",
    "#     if '__interrupt__' in event:\n",
    "#         interrupt_value = event['__interrupt__'][0].value\n",
    "#         display(Markdown(interrupt_value))"
   ],
   "id": "37c5fad018809e10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "{'load_file': {'columns_info': 'Index: SNAPSHOT_TIME\\nColumns: HOST, PORT, PING_MS, CPU, USED_GB, SWAP_MB, CONNS, TRANS, WAIT_THR, BUSY_SQL, PEND_SESS, D_WR_MBPS, DOWN_EVENT'}}\n",
      "================================\n",
      "{'router': None}\n",
      "================================\n",
      "{'create_tool_agent_builder': {'code_approval_items': [CodeApprovalItem(code='import pandas as pd\\nfrom sklearn.ensemble import IsolationForest\\nimport matplotlib.pyplot as plt\\n\\n\\ndef analyze_time_series(file_path):\\n    \"\"\"\\n    This function loads a CSV file, parses its \\'SNAPSHOT_TIME\\' column as datetime, sets it as the index, \\n    and then analyzes time series data for the metrics PING_MS, CPU, USED_GB, and CONNS.\\n    For each metric, it applies an Isolation Forest algorithm for anomaly detection, plots the time series \\n    along with the detected anomalies, and returns a dictionary containing both the full series and the \\n    detected anomalies.\\n    \\n    Parameters:\\n    file_path (str): The path to the CSV file to be loaded.\\n    \\n    Returns:\\n    dict: Contains the time series and anomaly information for each metric.\\n    \"\"\"\\n    # Load data\\n    df = pd.read_csv(file_path)\\n\\n    # Convert SNAPSHOT_TIME to datetime\\n    df[\\'SNAPSHOT_TIME\\'] = pd.to_datetime(df[\\'SNAPSHOT_TIME\\'])\\n    \\n    # Set SNAPSHOT_TIME as index\\n    df.set_index(\\'SNAPSHOT_TIME\\', inplace=True)\\n\\n    # Define the metrics to analyze\\n    metrics = [\\'PING_MS\\', \\'CPU\\', \\'USED_GB\\', \\'CONNS\\']\\n\\n    results = {}\\n    \\n    for metric in metrics:\\n        # Ensure no missing values for the metric\\n        series = df[metric].dropna()\\n        \\n        # Anomaly detection using IsolationForest\\n        model = IsolationForest(contamination=0.01, random_state=42)\\n        reshaped = series.values.reshape(-1, 1)\\n        model.fit(reshaped)\\n        predictions = model.predict(reshaped)\\n\\n        # Extract the anomaly points (IsolationForest returns -1 for anomalies)\\n        anomalies = series[predictions == -1]\\n\\n        results[metric] = {\\n            \\'series\\': series,\\n            \\'anomalies\\': anomalies\\n        }\\n\\n        # Plot the time series along with the detected anomalies\\n        plt.figure(figsize=(12, 4))\\n        plt.plot(series.index, series.values, label=\\'Time Series\\')\\n        plt.scatter(anomalies.index, anomalies.values, color=\\'red\\', label=\\'Anomalies\\')\\n        plt.title(f\\'{metric} over Time with Anomalies\\')\\n        plt.xlabel(\\'Time\\')\\n        plt.ylabel(metric)\\n        plt.legend()\\n        plt.tight_layout()\\n        plt.show()\\n\\n    return results\\n\\n\\nif __name__ == \\'__main__\\':\\n    # This is for testing purposes. Replace \\'data.csv\\' with the actual file path.\\n    file_path = \\'data.csv\\'\\n    result = analyze_time_series(file_path)\\n    for metric, data in result.items():\\n        print(f\"Metric: {metric}\")\\n        print(f\"Total Data Points: {len(data[\\'series\\'])}\")\\n        print(f\"Anomalies Detected: {len(data[\\'anomalies\\'])}\")\\n        print(\\'\\')', approval='true'), CodeApprovalItem(code='def create_correlation_matrix(file_path):\\n    import pandas as pd\\n    import seaborn as sns\\n    import matplotlib.pyplot as plt\\n\\n    # Load data from the given file path\\n    df = pd.read_csv(file_path)\\n\\n    # Define the columns of interest\\n    columns_of_interest = [\\'CPU\\', \\'USED_GB\\', \\'SWAP_MB\\', \\'BUSY_SQL\\']\\n\\n    # Check that all required columns are present\\n    missing_columns = [col for col in columns_of_interest if col not in df.columns]\\n    if missing_columns:\\n        raise ValueError(f\"Missing columns in data file: {missing_columns}\")\\n\\n    # Calculate the correlation matrix of the selected columns\\n    corr_matrix = df[columns_of_interest].corr()\\n\\n    # Create a heatmap for the correlation matrix\\n    plt.figure(figsize=(8, 6))\\n    sns.heatmap(corr_matrix, annot=True, cmap=\\'coolwarm\\')\\n    plt.title(\\'Correlation Matrix of System Metrics\\')\\n    plt.xlabel(\\'Metrics\\')\\n    plt.ylabel(\\'Metrics\\')\\n    plt.tight_layout()\\n    plt.show()\\n\\n# Example usage:\\n# create_correlation_matrix(\\'your_file.csv\\')', approval='true'), CodeApprovalItem(code='import pandas as pd\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.preprocessing import StandardScaler\\nimport matplotlib.pyplot as plt\\n\\n\\ndef cluster_system_performance(file_path):\\n    \"\"\"\\n    Reads system performance data from a CSV file, performs clustering to categorize snapshots into\\n    different operational states and saves the resulting dataframe with cluster labels to a new CSV file.\\n    \\n    Parameters:\\n    file_path (str): Path to the input CSV file containing system performance data.\\n    \\n    Returns:\\n    DataFrame: The original dataframe augmented with a \\'Performance_Profile\\' column indicating cluster labels.\\n    \"\"\"\\n    # Load data from the provided CSV file\\n    df = pd.read_csv(file_path)\\n\\n    # Select features for clustering\\n    features = [\\'PING_MS\\', \\'CPU\\', \\'USED_GB\\', \\'SWAP_MB\\', \\'CONNS\\', \\'TRANS\\', \\'WAIT_THR\\', \\'BUSY_SQL\\', \\'PEND_SESS\\', \\'D_WR_MBPS\\', \\'DOWN_EVENT\\']\\n    X = df[features].fillna(0)  # Handle missing values if any\\n\\n    # Standardize features\\n    scaler = StandardScaler()\\n    X_scaled = scaler.fit_transform(X)\\n\\n    # Determine optimal number of clusters using the elbow method\\n    wcss = []\\n    for i in range(1, 11):\\n        kmeans = KMeans(n_clusters=i, random_state=42)\\n        kmeans.fit(X_scaled)\\n        wcss.append(kmeans.inertia_)\\n\\n    # Optional: Plot elbow curve to choose the number of clusters\\n    plt.figure()\\n    plt.plot(range(1, 11), wcss, marker=\\'o\\')\\n    plt.xlabel(\\'Number of clusters\\')\\n    plt.ylabel(\\'Within-cluster Sum of Squares (WCSS)\\')\\n    plt.title(\\'Elbow Method for Optimal k\\')\\n    plt.show()\\n\\n    # From the elbow plot, choose the optimal number of clusters, e.g., 3\\n    optimal_k = 3\\n    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\\n    clusters = kmeans.fit_predict(X_scaled)\\n\\n    # Add cluster labels to the original dataframe\\n    df[\\'Performance_Profile\\'] = clusters\\n\\n    # Save the result to a new CSV file\\n    output_file = \\'system_performance_clusters.csv\\'\\n    df.to_csv(output_file, index=False)\\n    print(f\\'Results saved to {output_file}\\')\\n\\n    return df\\n\\n\\n# Example usage:\\n# df_result = cluster_system_performance(\\'system_performance_data.csv\\')\\n', approval='true'), [CodeApprovalItem(code='import pandas as pd\\nfrom sklearn.ensemble import IsolationForest\\nimport matplotlib.pyplot as plt\\n\\n\\ndef analyze_time_series(file_path):\\n    \"\"\"\\n    This function loads a CSV file, parses its \\'SNAPSHOT_TIME\\' column as datetime, sets it as the index, \\n    and then analyzes time series data for the metrics PING_MS, CPU, USED_GB, and CONNS.\\n    For each metric, it applies an Isolation Forest algorithm for anomaly detection, plots the time series \\n    along with the detected anomalies, and returns a dictionary containing both the full series and the \\n    detected anomalies.\\n    \\n    Parameters:\\n    file_path (str): The path to the CSV file to be loaded.\\n    \\n    Returns:\\n    dict: Contains the time series and anomaly information for each metric.\\n    \"\"\"\\n    # Load data\\n    df = pd.read_csv(file_path)\\n\\n    # Convert SNAPSHOT_TIME to datetime\\n    df[\\'SNAPSHOT_TIME\\'] = pd.to_datetime(df[\\'SNAPSHOT_TIME\\'])\\n    \\n    # Set SNAPSHOT_TIME as index\\n    df.set_index(\\'SNAPSHOT_TIME\\', inplace=True)\\n\\n    # Define the metrics to analyze\\n    metrics = [\\'PING_MS\\', \\'CPU\\', \\'USED_GB\\', \\'CONNS\\']\\n\\n    results = {}\\n    \\n    for metric in metrics:\\n        # Ensure no missing values for the metric\\n        series = df[metric].dropna()\\n        \\n        # Anomaly detection using IsolationForest\\n        model = IsolationForest(contamination=0.01, random_state=42)\\n        reshaped = series.values.reshape(-1, 1)\\n        model.fit(reshaped)\\n        predictions = model.predict(reshaped)\\n\\n        # Extract the anomaly points (IsolationForest returns -1 for anomalies)\\n        anomalies = series[predictions == -1]\\n\\n        results[metric] = {\\n            \\'series\\': series,\\n            \\'anomalies\\': anomalies\\n        }\\n\\n        # Plot the time series along with the detected anomalies\\n        plt.figure(figsize=(12, 4))\\n        plt.plot(series.index, series.values, label=\\'Time Series\\')\\n        plt.scatter(anomalies.index, anomalies.values, color=\\'red\\', label=\\'Anomalies\\')\\n        plt.title(f\\'{metric} over Time with Anomalies\\')\\n        plt.xlabel(\\'Time\\')\\n        plt.ylabel(metric)\\n        plt.legend()\\n        plt.tight_layout()\\n        plt.show()\\n\\n    return results\\n\\n\\nif __name__ == \\'__main__\\':\\n    # This is for testing purposes. Replace \\'data.csv\\' with the actual file path.\\n    file_path = \\'data.csv\\'\\n    result = analyze_time_series(file_path)\\n    for metric, data in result.items():\\n        print(f\"Metric: {metric}\")\\n        print(f\"Total Data Points: {len(data[\\'series\\'])}\")\\n        print(f\"Anomalies Detected: {len(data[\\'anomalies\\'])}\")\\n        print(\\'\\')', approval='true'), CodeApprovalItem(code='def create_correlation_matrix(file_path):\\n    import pandas as pd\\n    import seaborn as sns\\n    import matplotlib.pyplot as plt\\n\\n    # Load data from the given file path\\n    df = pd.read_csv(file_path)\\n\\n    # Define the columns of interest\\n    columns_of_interest = [\\'CPU\\', \\'USED_GB\\', \\'SWAP_MB\\', \\'BUSY_SQL\\']\\n\\n    # Check that all required columns are present\\n    missing_columns = [col for col in columns_of_interest if col not in df.columns]\\n    if missing_columns:\\n        raise ValueError(f\"Missing columns in data file: {missing_columns}\")\\n\\n    # Calculate the correlation matrix of the selected columns\\n    corr_matrix = df[columns_of_interest].corr()\\n\\n    # Create a heatmap for the correlation matrix\\n    plt.figure(figsize=(8, 6))\\n    sns.heatmap(corr_matrix, annot=True, cmap=\\'coolwarm\\')\\n    plt.title(\\'Correlation Matrix of System Metrics\\')\\n    plt.xlabel(\\'Metrics\\')\\n    plt.ylabel(\\'Metrics\\')\\n    plt.tight_layout()\\n    plt.show()\\n\\n# Example usage:\\n# create_correlation_matrix(\\'your_file.csv\\')', approval='true'), CodeApprovalItem(code='import pandas as pd\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.preprocessing import StandardScaler\\nimport matplotlib.pyplot as plt\\n\\n\\ndef cluster_system_performance(file_path):\\n    \"\"\"\\n    Reads system performance data from a CSV file, performs clustering to categorize snapshots into\\n    different operational states and saves the resulting dataframe with cluster labels to a new CSV file.\\n    \\n    Parameters:\\n    file_path (str): Path to the input CSV file containing system performance data.\\n    \\n    Returns:\\n    DataFrame: The original dataframe augmented with a \\'Performance_Profile\\' column indicating cluster labels.\\n    \"\"\"\\n    # Load data from the provided CSV file\\n    df = pd.read_csv(file_path)\\n\\n    # Select features for clustering\\n    features = [\\'PING_MS\\', \\'CPU\\', \\'USED_GB\\', \\'SWAP_MB\\', \\'CONNS\\', \\'TRANS\\', \\'WAIT_THR\\', \\'BUSY_SQL\\', \\'PEND_SESS\\', \\'D_WR_MBPS\\', \\'DOWN_EVENT\\']\\n    X = df[features].fillna(0)  # Handle missing values if any\\n\\n    # Standardize features\\n    scaler = StandardScaler()\\n    X_scaled = scaler.fit_transform(X)\\n\\n    # Determine optimal number of clusters using the elbow method\\n    wcss = []\\n    for i in range(1, 11):\\n        kmeans = KMeans(n_clusters=i, random_state=42)\\n        kmeans.fit(X_scaled)\\n        wcss.append(kmeans.inertia_)\\n\\n    # Optional: Plot elbow curve to choose the number of clusters\\n    plt.figure()\\n    plt.plot(range(1, 11), wcss, marker=\\'o\\')\\n    plt.xlabel(\\'Number of clusters\\')\\n    plt.ylabel(\\'Within-cluster Sum of Squares (WCSS)\\')\\n    plt.title(\\'Elbow Method for Optimal k\\')\\n    plt.show()\\n\\n    # From the elbow plot, choose the optimal number of clusters, e.g., 3\\n    optimal_k = 3\\n    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\\n    clusters = kmeans.fit_predict(X_scaled)\\n\\n    # Add cluster labels to the original dataframe\\n    df[\\'Performance_Profile\\'] = clusters\\n\\n    # Save the result to a new CSV file\\n    output_file = \\'system_performance_clusters.csv\\'\\n    df.to_csv(output_file, index=False)\\n    print(f\\'Results saved to {output_file}\\')\\n\\n    return df\\n\\n\\n# Example usage:\\n# df_result = cluster_system_performance(\\'system_performance_data.csv\\')\\n', approval='true')]]}}\n",
      "================================\n",
      "{'create_tool_agent_builder': {'code_approval_items': [CodeApprovalItem(code='import pandas as pd\\nimport matplotlib.pyplot as plt\\nimport matplotlib.dates as mdates\\n\\n\\ndef analyze_network_performance(file_path):\\n    \"\"\"\\n    Analyzes network performance metrics from a CSV file and generates time series plots to show raw,\\n    smoothed data (via moving average), and anomalies based on 3 standard deviations for PING_MS and CONNS.\\n\\n    The CSV file is expected to have a header containing at least the following columns:\\n    - SNAPSHOT_TIME (dates, will be parsed)\\n    - PING_MS\\n    - CONNS\\n    \\n    Parameters:\\n    file_path (str): The path to the CSV data file.\\n    \"\"\"\\n    # Read the data assuming CSV format with header row and parse \\'SNAPSHOT_TIME\\' column as datetime\\n    df = pd.read_csv(file_path, parse_dates=[\\'SNAPSHOT_TIME\\'])\\n    \\n    # Set SNAPSHOT_TIME as the index for time series analysis\\n    df.set_index(\\'SNAPSHOT_TIME\\', inplace=True)\\n    \\n    # Plot PING_MS over time\\n    plt.figure(figsize=(14, 6))\\n    plt.plot(df.index, df[\\'PING_MS\\'], label=\\'PING_MS\\')\\n    plt.title(\\'Network PING_MS Over Time\\')\\n    plt.xlabel(\\'Time\\')\\n    plt.ylabel(\\'PING_MS (ms)\\')\\n    plt.legend()\\n    plt.grid(True)\\n    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\\'%Y-%m-%d %H:%M\\'))\\n    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\\n    plt.gcf().autofmt_xdate()\\n    plt.show()\\n    \\n    # Plot CONNS over time\\n    plt.figure(figsize=(14, 6))\\n    plt.plot(df.index, df[\\'CONNS\\'], label=\\'CONNS\\')\\n    plt.title(\\'Network CONNS Over Time\\')\\n    plt.xlabel(\\'Time\\')\\n    plt.ylabel(\\'Number of Connections\\')\\n    plt.legend()\\n    plt.grid(True)\\n    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\\'%Y-%m-%d %H:%M\\'))\\n    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\\n    plt.gcf().autofmt_xdate()\\n    plt.show()\\n    \\n    # Calculate moving averages (using a window of 10 data points)\\n    df[\\'PING_MS_MA\\'] = df[\\'PING_MS\\'].rolling(window=10).mean()\\n    df[\\'CONNS_MA\\'] = df[\\'CONNS\\'].rolling(window=10).mean()\\n    \\n    # Plot smoothed PING_MS with moving average\\n    plt.figure(figsize=(14, 6))\\n    plt.plot(df.index, df[\\'PING_MS\\'], alpha=0.3, label=\\'PING_MS Raw\\')\\n    plt.plot(df.index, df[\\'PING_MS_MA\\'], label=\\'PING_MS Moving Avg\\', linewidth=2)\\n    plt.title(\\'Smoothed Network PING_MS\\')\\n    plt.xlabel(\\'Time\\')\\n    plt.ylabel(\\'PING_MS (ms)\\')\\n    plt.legend()\\n    plt.grid(True)\\n    plt.show()\\n    \\n    # Plot smoothed CONNS with moving average\\n    plt.figure(figsize=(14, 6))\\n    plt.plot(df.index, df[\\'CONNS\\'], alpha=0.3, label=\\'CONNS Raw\\')\\n    plt.plot(df.index, df[\\'CONNS_MA\\'], label=\\'CONNS Moving Avg\\', linewidth=2)\\n    plt.title(\\'Smoothed Network CONNS\\')\\n    plt.xlabel(\\'Time\\')\\n    plt.ylabel(\\'Number of Connections\\')\\n    plt.legend()\\n    plt.grid(True)\\n    plt.show()\\n    \\n    # Detect anomalies using a threshold of 3 standard deviations from the mean\\n    ping_mean = df[\\'PING_MS\\'].mean()\\n    ping_std = df[\\'PING_MS\\'].std()\\n    conns_mean = df[\\'CONNS\\'].mean()\\n    conns_std = df[\\'CONNS\\'].std()\\n    \\n    df[\\'PING_MS_anomaly\\'] = (df[\\'PING_MS\\'] > ping_mean + 3 * ping_std) | (df[\\'PING_MS\\'] < ping_mean - 3 * ping_std)\\n    df[\\'CONNS_anomaly\\'] = (df[\\'CONNS\\'] > conns_mean + 3 * conns_std) | (df[\\'CONNS\\'] < conns_mean - 3 * conns_std)\\n    \\n    # Plot anomalies detected in PING_MS\\n    plt.figure(figsize=(14, 6))\\n    colors = df[\\'PING_MS_anomaly\\'].apply(lambda x: \\'red\\' if x else \\'blue\\')\\n    plt.scatter(df.index, df[\\'PING_MS\\'], c=colors, label=\\'PING_MS Anomaly Points\\')\\n    plt.title(\\'PING_MS with Anomalies\\')\\n    plt.xlabel(\\'Time\\')\\n    plt.ylabel(\\'PING_MS (ms)\\')\\n    plt.legend()\\n    plt.grid(True)\\n    plt.show()\\n    \\n    # Plot anomalies detected in CONNS\\n    plt.figure(figsize=(14, 6))\\n    colors = df[\\'CONNS_anomaly\\'].apply(lambda x: \\'red\\' if x else \\'blue\\')\\n    plt.scatter(df.index, df[\\'CONNS\\'], c=colors, label=\\'CONNS Anomaly Points\\')\\n    plt.title(\\'CONNS with Anomalies\\')\\n    plt.xlabel(\\'Time\\')\\n    plt.ylabel(\\'Number of Connections\\')\\n    plt.legend()\\n    plt.grid(True)\\n    plt.show()\\n\\n\\n# Example usage:\\n# analyze_network_performance(\\'path_to_your_data.csv\\')\\n\\n\\nif __name__ == \\'__main__\\':\\n    # Example: Replace \\'data.csv\\' with your file path to run the analysis.\\n    file_path = \\'data.csv\\'\\n    analyze_network_performance(file_path)', approval='true'), CodeApprovalItem(code='import pandas as pd\\n\\ndef create_feature_engineering_pipeline(file_path):\\n    \"\"\"Load data from a CSV file and compute aggregated statistics (mean, median, max, min) for specified numerical columns grouped by \\'HOST\\' and \\'PORT\\'.\"\"\"\\n    # Load the dataset from the file\\n    df = pd.read_csv(file_path)\\n    \\n    # Specify the numerical columns to analyze\\n    numerical_cols = [\\'PING_MS\\', \\'CPU\\', \\'USED_GB\\', \\'SWAP_MB\\', \\'CONNS\\', \\'TRANS\\', \\'WAIT_THR\\', \\'BUSY_SQL\\', \\'PEND_SESS\\', \\'D_WR_MBPS\\', \\'DOWN_EVENT\\']\\n    \\n    # Group by \\'HOST\\' and \\'PORT\\'\\n    grouped = df.groupby([\\'HOST\\', \\'PORT\\'])\\n    \\n    # Calculate aggregated statistics\\n    aggregated = grouped[numerical_cols].agg([\\'mean\\', \\'median\\', \\'max\\', \\'min\\'])\\n    \\n    # Flatten multi-level columns\\n    aggregated.columns = [\\'_\\'.join(col).strip() for col in aggregated.columns.values]\\n    \\n    # Reset index to turn grouping columns back into columns\\n    summary_df = aggregated.reset_index()\\n    \\n    return summary_df\\n\\n# Example usage:\\n# summary = create_feature_engineering_pipeline(\\'your_data.csv\\')\\n# print(summary)\\n', approval='true'), CodeApprovalItem(code='import pandas as pd\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.preprocessing import StandardScaler\\n\\n\\ndef cluster_hosts(file_path):\\n    \"\"\"\\n    Load host resource utilization and network activity data from a CSV file, \\n    standardize the features, and cluster the hosts using KMeans.\\n    \\n    Args:\\n        file_path (str): Path to the CSV data file.\\n    \\n    Returns:\\n        DataFrame: The original DataFrame with an additional \\'Cluster\\' column representing the cluster assignment.\\n    \"\"\"\\n    # Load data from the file\\n    df = pd.read_csv(file_path, parse_dates=[\\'SNAPSHOT_TIME\\'])\\n\\n    # Select features for clustering (excluding non-numeric or identifier columns)\\n    features = [\\'CPU\\', \\'USED_GB\\', \\'SWAP_MB\\', \\'CONNS\\', \\'TRANS\\', \\'WAIT_THR\\']\\n    X = df[features].fillna(0)  # Fill NaN values if any\\n\\n    # Standardize features\\n    scaler = StandardScaler()\\n    X_scaled = scaler.fit_transform(X)\\n\\n    # Determine the optimal number of clusters using the elbow method (here simplified by assuming 3 clusters)\\n    # In practice, you might want to analyze the distortions list to choose k.\\n    n_clusters = 3\\n\\n    # Fit final model using KMeans\\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\\n    clusters = kmeans.fit_predict(X_scaled)\\n\\n    # Add cluster labels to the original DataFrame\\n    df[\\'Cluster\\'] = clusters\\n    \\n    return df\\n\\n\\nif __name__ == \\'__main__\\':\\n    # Example usage\\n    file_path = \\'your_data_file.csv\\'  # Replace with your actual file path\\n    clustered_df = cluster_hosts(file_path)\\n    print(clustered_df.head())\\n', approval='true'), [CodeApprovalItem(code='import pandas as pd\\nimport matplotlib.pyplot as plt\\nimport matplotlib.dates as mdates\\n\\n\\ndef analyze_network_performance(file_path):\\n    \"\"\"\\n    Analyzes network performance metrics from a CSV file and generates time series plots to show raw,\\n    smoothed data (via moving average), and anomalies based on 3 standard deviations for PING_MS and CONNS.\\n\\n    The CSV file is expected to have a header containing at least the following columns:\\n    - SNAPSHOT_TIME (dates, will be parsed)\\n    - PING_MS\\n    - CONNS\\n    \\n    Parameters:\\n    file_path (str): The path to the CSV data file.\\n    \"\"\"\\n    # Read the data assuming CSV format with header row and parse \\'SNAPSHOT_TIME\\' column as datetime\\n    df = pd.read_csv(file_path, parse_dates=[\\'SNAPSHOT_TIME\\'])\\n    \\n    # Set SNAPSHOT_TIME as the index for time series analysis\\n    df.set_index(\\'SNAPSHOT_TIME\\', inplace=True)\\n    \\n    # Plot PING_MS over time\\n    plt.figure(figsize=(14, 6))\\n    plt.plot(df.index, df[\\'PING_MS\\'], label=\\'PING_MS\\')\\n    plt.title(\\'Network PING_MS Over Time\\')\\n    plt.xlabel(\\'Time\\')\\n    plt.ylabel(\\'PING_MS (ms)\\')\\n    plt.legend()\\n    plt.grid(True)\\n    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\\'%Y-%m-%d %H:%M\\'))\\n    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\\n    plt.gcf().autofmt_xdate()\\n    plt.show()\\n    \\n    # Plot CONNS over time\\n    plt.figure(figsize=(14, 6))\\n    plt.plot(df.index, df[\\'CONNS\\'], label=\\'CONNS\\')\\n    plt.title(\\'Network CONNS Over Time\\')\\n    plt.xlabel(\\'Time\\')\\n    plt.ylabel(\\'Number of Connections\\')\\n    plt.legend()\\n    plt.grid(True)\\n    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\\'%Y-%m-%d %H:%M\\'))\\n    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\\n    plt.gcf().autofmt_xdate()\\n    plt.show()\\n    \\n    # Calculate moving averages (using a window of 10 data points)\\n    df[\\'PING_MS_MA\\'] = df[\\'PING_MS\\'].rolling(window=10).mean()\\n    df[\\'CONNS_MA\\'] = df[\\'CONNS\\'].rolling(window=10).mean()\\n    \\n    # Plot smoothed PING_MS with moving average\\n    plt.figure(figsize=(14, 6))\\n    plt.plot(df.index, df[\\'PING_MS\\'], alpha=0.3, label=\\'PING_MS Raw\\')\\n    plt.plot(df.index, df[\\'PING_MS_MA\\'], label=\\'PING_MS Moving Avg\\', linewidth=2)\\n    plt.title(\\'Smoothed Network PING_MS\\')\\n    plt.xlabel(\\'Time\\')\\n    plt.ylabel(\\'PING_MS (ms)\\')\\n    plt.legend()\\n    plt.grid(True)\\n    plt.show()\\n    \\n    # Plot smoothed CONNS with moving average\\n    plt.figure(figsize=(14, 6))\\n    plt.plot(df.index, df[\\'CONNS\\'], alpha=0.3, label=\\'CONNS Raw\\')\\n    plt.plot(df.index, df[\\'CONNS_MA\\'], label=\\'CONNS Moving Avg\\', linewidth=2)\\n    plt.title(\\'Smoothed Network CONNS\\')\\n    plt.xlabel(\\'Time\\')\\n    plt.ylabel(\\'Number of Connections\\')\\n    plt.legend()\\n    plt.grid(True)\\n    plt.show()\\n    \\n    # Detect anomalies using a threshold of 3 standard deviations from the mean\\n    ping_mean = df[\\'PING_MS\\'].mean()\\n    ping_std = df[\\'PING_MS\\'].std()\\n    conns_mean = df[\\'CONNS\\'].mean()\\n    conns_std = df[\\'CONNS\\'].std()\\n    \\n    df[\\'PING_MS_anomaly\\'] = (df[\\'PING_MS\\'] > ping_mean + 3 * ping_std) | (df[\\'PING_MS\\'] < ping_mean - 3 * ping_std)\\n    df[\\'CONNS_anomaly\\'] = (df[\\'CONNS\\'] > conns_mean + 3 * conns_std) | (df[\\'CONNS\\'] < conns_mean - 3 * conns_std)\\n    \\n    # Plot anomalies detected in PING_MS\\n    plt.figure(figsize=(14, 6))\\n    colors = df[\\'PING_MS_anomaly\\'].apply(lambda x: \\'red\\' if x else \\'blue\\')\\n    plt.scatter(df.index, df[\\'PING_MS\\'], c=colors, label=\\'PING_MS Anomaly Points\\')\\n    plt.title(\\'PING_MS with Anomalies\\')\\n    plt.xlabel(\\'Time\\')\\n    plt.ylabel(\\'PING_MS (ms)\\')\\n    plt.legend()\\n    plt.grid(True)\\n    plt.show()\\n    \\n    # Plot anomalies detected in CONNS\\n    plt.figure(figsize=(14, 6))\\n    colors = df[\\'CONNS_anomaly\\'].apply(lambda x: \\'red\\' if x else \\'blue\\')\\n    plt.scatter(df.index, df[\\'CONNS\\'], c=colors, label=\\'CONNS Anomaly Points\\')\\n    plt.title(\\'CONNS with Anomalies\\')\\n    plt.xlabel(\\'Time\\')\\n    plt.ylabel(\\'Number of Connections\\')\\n    plt.legend()\\n    plt.grid(True)\\n    plt.show()\\n\\n\\n# Example usage:\\n# analyze_network_performance(\\'path_to_your_data.csv\\')\\n\\n\\nif __name__ == \\'__main__\\':\\n    # Example: Replace \\'data.csv\\' with your file path to run the analysis.\\n    file_path = \\'data.csv\\'\\n    analyze_network_performance(file_path)', approval='true'), CodeApprovalItem(code='import pandas as pd\\n\\ndef create_feature_engineering_pipeline(file_path):\\n    \"\"\"Load data from a CSV file and compute aggregated statistics (mean, median, max, min) for specified numerical columns grouped by \\'HOST\\' and \\'PORT\\'.\"\"\"\\n    # Load the dataset from the file\\n    df = pd.read_csv(file_path)\\n    \\n    # Specify the numerical columns to analyze\\n    numerical_cols = [\\'PING_MS\\', \\'CPU\\', \\'USED_GB\\', \\'SWAP_MB\\', \\'CONNS\\', \\'TRANS\\', \\'WAIT_THR\\', \\'BUSY_SQL\\', \\'PEND_SESS\\', \\'D_WR_MBPS\\', \\'DOWN_EVENT\\']\\n    \\n    # Group by \\'HOST\\' and \\'PORT\\'\\n    grouped = df.groupby([\\'HOST\\', \\'PORT\\'])\\n    \\n    # Calculate aggregated statistics\\n    aggregated = grouped[numerical_cols].agg([\\'mean\\', \\'median\\', \\'max\\', \\'min\\'])\\n    \\n    # Flatten multi-level columns\\n    aggregated.columns = [\\'_\\'.join(col).strip() for col in aggregated.columns.values]\\n    \\n    # Reset index to turn grouping columns back into columns\\n    summary_df = aggregated.reset_index()\\n    \\n    return summary_df\\n\\n# Example usage:\\n# summary = create_feature_engineering_pipeline(\\'your_data.csv\\')\\n# print(summary)\\n', approval='true'), CodeApprovalItem(code='import pandas as pd\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.preprocessing import StandardScaler\\n\\n\\ndef cluster_hosts(file_path):\\n    \"\"\"\\n    Load host resource utilization and network activity data from a CSV file, \\n    standardize the features, and cluster the hosts using KMeans.\\n    \\n    Args:\\n        file_path (str): Path to the CSV data file.\\n    \\n    Returns:\\n        DataFrame: The original DataFrame with an additional \\'Cluster\\' column representing the cluster assignment.\\n    \"\"\"\\n    # Load data from the file\\n    df = pd.read_csv(file_path, parse_dates=[\\'SNAPSHOT_TIME\\'])\\n\\n    # Select features for clustering (excluding non-numeric or identifier columns)\\n    features = [\\'CPU\\', \\'USED_GB\\', \\'SWAP_MB\\', \\'CONNS\\', \\'TRANS\\', \\'WAIT_THR\\']\\n    X = df[features].fillna(0)  # Fill NaN values if any\\n\\n    # Standardize features\\n    scaler = StandardScaler()\\n    X_scaled = scaler.fit_transform(X)\\n\\n    # Determine the optimal number of clusters using the elbow method (here simplified by assuming 3 clusters)\\n    # In practice, you might want to analyze the distortions list to choose k.\\n    n_clusters = 3\\n\\n    # Fit final model using KMeans\\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\\n    clusters = kmeans.fit_predict(X_scaled)\\n\\n    # Add cluster labels to the original DataFrame\\n    df[\\'Cluster\\'] = clusters\\n    \\n    return df\\n\\n\\nif __name__ == \\'__main__\\':\\n    # Example usage\\n    file_path = \\'your_data_file.csv\\'  # Replace with your actual file path\\n    clustered_df = cluster_hosts(file_path)\\n    print(clustered_df.head())\\n', approval='true')]]}}\n",
      "================================\n",
      "{'space_holder': {'orig_file_path': 'C:\\\\Users\\\\kaiyi\\\\Desktop\\\\github\\\\automatic-tool-generation\\\\dataset\\\\synthetic_server_metrics.csv', 'columns_info': 'Index: SNAPSHOT_TIME\\nColumns: HOST, PORT, PING_MS, CPU, USED_GB, SWAP_MB, CONNS, TRANS, WAIT_THR, BUSY_SQL, PEND_SESS, D_WR_MBPS, DOWN_EVENT', 'code_approval_items': [CodeApprovalItem(code='import pandas as pd\\nfrom sklearn.ensemble import IsolationForest\\nimport matplotlib.pyplot as plt\\n\\n\\ndef analyze_time_series(file_path):\\n    \"\"\"\\n    This function loads a CSV file, parses its \\'SNAPSHOT_TIME\\' column as datetime, sets it as the index, \\n    and then analyzes time series data for the metrics PING_MS, CPU, USED_GB, and CONNS.\\n    For each metric, it applies an Isolation Forest algorithm for anomaly detection, plots the time series \\n    along with the detected anomalies, and returns a dictionary containing both the full series and the \\n    detected anomalies.\\n    \\n    Parameters:\\n    file_path (str): The path to the CSV file to be loaded.\\n    \\n    Returns:\\n    dict: Contains the time series and anomaly information for each metric.\\n    \"\"\"\\n    # Load data\\n    df = pd.read_csv(file_path)\\n\\n    # Convert SNAPSHOT_TIME to datetime\\n    df[\\'SNAPSHOT_TIME\\'] = pd.to_datetime(df[\\'SNAPSHOT_TIME\\'])\\n    \\n    # Set SNAPSHOT_TIME as index\\n    df.set_index(\\'SNAPSHOT_TIME\\', inplace=True)\\n\\n    # Define the metrics to analyze\\n    metrics = [\\'PING_MS\\', \\'CPU\\', \\'USED_GB\\', \\'CONNS\\']\\n\\n    results = {}\\n    \\n    for metric in metrics:\\n        # Ensure no missing values for the metric\\n        series = df[metric].dropna()\\n        \\n        # Anomaly detection using IsolationForest\\n        model = IsolationForest(contamination=0.01, random_state=42)\\n        reshaped = series.values.reshape(-1, 1)\\n        model.fit(reshaped)\\n        predictions = model.predict(reshaped)\\n\\n        # Extract the anomaly points (IsolationForest returns -1 for anomalies)\\n        anomalies = series[predictions == -1]\\n\\n        results[metric] = {\\n            \\'series\\': series,\\n            \\'anomalies\\': anomalies\\n        }\\n\\n        # Plot the time series along with the detected anomalies\\n        plt.figure(figsize=(12, 4))\\n        plt.plot(series.index, series.values, label=\\'Time Series\\')\\n        plt.scatter(anomalies.index, anomalies.values, color=\\'red\\', label=\\'Anomalies\\')\\n        plt.title(f\\'{metric} over Time with Anomalies\\')\\n        plt.xlabel(\\'Time\\')\\n        plt.ylabel(metric)\\n        plt.legend()\\n        plt.tight_layout()\\n        plt.show()\\n\\n    return results\\n\\n\\nif __name__ == \\'__main__\\':\\n    # This is for testing purposes. Replace \\'data.csv\\' with the actual file path.\\n    file_path = \\'data.csv\\'\\n    result = analyze_time_series(file_path)\\n    for metric, data in result.items():\\n        print(f\"Metric: {metric}\")\\n        print(f\"Total Data Points: {len(data[\\'series\\'])}\")\\n        print(f\"Anomalies Detected: {len(data[\\'anomalies\\'])}\")\\n        print(\\'\\')', approval='true'), CodeApprovalItem(code='def create_correlation_matrix(file_path):\\n    import pandas as pd\\n    import seaborn as sns\\n    import matplotlib.pyplot as plt\\n\\n    # Load data from the given file path\\n    df = pd.read_csv(file_path)\\n\\n    # Define the columns of interest\\n    columns_of_interest = [\\'CPU\\', \\'USED_GB\\', \\'SWAP_MB\\', \\'BUSY_SQL\\']\\n\\n    # Check that all required columns are present\\n    missing_columns = [col for col in columns_of_interest if col not in df.columns]\\n    if missing_columns:\\n        raise ValueError(f\"Missing columns in data file: {missing_columns}\")\\n\\n    # Calculate the correlation matrix of the selected columns\\n    corr_matrix = df[columns_of_interest].corr()\\n\\n    # Create a heatmap for the correlation matrix\\n    plt.figure(figsize=(8, 6))\\n    sns.heatmap(corr_matrix, annot=True, cmap=\\'coolwarm\\')\\n    plt.title(\\'Correlation Matrix of System Metrics\\')\\n    plt.xlabel(\\'Metrics\\')\\n    plt.ylabel(\\'Metrics\\')\\n    plt.tight_layout()\\n    plt.show()\\n\\n# Example usage:\\n# create_correlation_matrix(\\'your_file.csv\\')', approval='true'), CodeApprovalItem(code='import pandas as pd\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.preprocessing import StandardScaler\\nimport matplotlib.pyplot as plt\\n\\n\\ndef cluster_system_performance(file_path):\\n    \"\"\"\\n    Reads system performance data from a CSV file, performs clustering to categorize snapshots into\\n    different operational states and saves the resulting dataframe with cluster labels to a new CSV file.\\n    \\n    Parameters:\\n    file_path (str): Path to the input CSV file containing system performance data.\\n    \\n    Returns:\\n    DataFrame: The original dataframe augmented with a \\'Performance_Profile\\' column indicating cluster labels.\\n    \"\"\"\\n    # Load data from the provided CSV file\\n    df = pd.read_csv(file_path)\\n\\n    # Select features for clustering\\n    features = [\\'PING_MS\\', \\'CPU\\', \\'USED_GB\\', \\'SWAP_MB\\', \\'CONNS\\', \\'TRANS\\', \\'WAIT_THR\\', \\'BUSY_SQL\\', \\'PEND_SESS\\', \\'D_WR_MBPS\\', \\'DOWN_EVENT\\']\\n    X = df[features].fillna(0)  # Handle missing values if any\\n\\n    # Standardize features\\n    scaler = StandardScaler()\\n    X_scaled = scaler.fit_transform(X)\\n\\n    # Determine optimal number of clusters using the elbow method\\n    wcss = []\\n    for i in range(1, 11):\\n        kmeans = KMeans(n_clusters=i, random_state=42)\\n        kmeans.fit(X_scaled)\\n        wcss.append(kmeans.inertia_)\\n\\n    # Optional: Plot elbow curve to choose the number of clusters\\n    plt.figure()\\n    plt.plot(range(1, 11), wcss, marker=\\'o\\')\\n    plt.xlabel(\\'Number of clusters\\')\\n    plt.ylabel(\\'Within-cluster Sum of Squares (WCSS)\\')\\n    plt.title(\\'Elbow Method for Optimal k\\')\\n    plt.show()\\n\\n    # From the elbow plot, choose the optimal number of clusters, e.g., 3\\n    optimal_k = 3\\n    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\\n    clusters = kmeans.fit_predict(X_scaled)\\n\\n    # Add cluster labels to the original dataframe\\n    df[\\'Performance_Profile\\'] = clusters\\n\\n    # Save the result to a new CSV file\\n    output_file = \\'system_performance_clusters.csv\\'\\n    df.to_csv(output_file, index=False)\\n    print(f\\'Results saved to {output_file}\\')\\n\\n    return df\\n\\n\\n# Example usage:\\n# df_result = cluster_system_performance(\\'system_performance_data.csv\\')\\n', approval='true'), [CodeApprovalItem(code='import pandas as pd\\nfrom sklearn.ensemble import IsolationForest\\nimport matplotlib.pyplot as plt\\n\\n\\ndef analyze_time_series(file_path):\\n    \"\"\"\\n    This function loads a CSV file, parses its \\'SNAPSHOT_TIME\\' column as datetime, sets it as the index, \\n    and then analyzes time series data for the metrics PING_MS, CPU, USED_GB, and CONNS.\\n    For each metric, it applies an Isolation Forest algorithm for anomaly detection, plots the time series \\n    along with the detected anomalies, and returns a dictionary containing both the full series and the \\n    detected anomalies.\\n    \\n    Parameters:\\n    file_path (str): The path to the CSV file to be loaded.\\n    \\n    Returns:\\n    dict: Contains the time series and anomaly information for each metric.\\n    \"\"\"\\n    # Load data\\n    df = pd.read_csv(file_path)\\n\\n    # Convert SNAPSHOT_TIME to datetime\\n    df[\\'SNAPSHOT_TIME\\'] = pd.to_datetime(df[\\'SNAPSHOT_TIME\\'])\\n    \\n    # Set SNAPSHOT_TIME as index\\n    df.set_index(\\'SNAPSHOT_TIME\\', inplace=True)\\n\\n    # Define the metrics to analyze\\n    metrics = [\\'PING_MS\\', \\'CPU\\', \\'USED_GB\\', \\'CONNS\\']\\n\\n    results = {}\\n    \\n    for metric in metrics:\\n        # Ensure no missing values for the metric\\n        series = df[metric].dropna()\\n        \\n        # Anomaly detection using IsolationForest\\n        model = IsolationForest(contamination=0.01, random_state=42)\\n        reshaped = series.values.reshape(-1, 1)\\n        model.fit(reshaped)\\n        predictions = model.predict(reshaped)\\n\\n        # Extract the anomaly points (IsolationForest returns -1 for anomalies)\\n        anomalies = series[predictions == -1]\\n\\n        results[metric] = {\\n            \\'series\\': series,\\n            \\'anomalies\\': anomalies\\n        }\\n\\n        # Plot the time series along with the detected anomalies\\n        plt.figure(figsize=(12, 4))\\n        plt.plot(series.index, series.values, label=\\'Time Series\\')\\n        plt.scatter(anomalies.index, anomalies.values, color=\\'red\\', label=\\'Anomalies\\')\\n        plt.title(f\\'{metric} over Time with Anomalies\\')\\n        plt.xlabel(\\'Time\\')\\n        plt.ylabel(metric)\\n        plt.legend()\\n        plt.tight_layout()\\n        plt.show()\\n\\n    return results\\n\\n\\nif __name__ == \\'__main__\\':\\n    # This is for testing purposes. Replace \\'data.csv\\' with the actual file path.\\n    file_path = \\'data.csv\\'\\n    result = analyze_time_series(file_path)\\n    for metric, data in result.items():\\n        print(f\"Metric: {metric}\")\\n        print(f\"Total Data Points: {len(data[\\'series\\'])}\")\\n        print(f\"Anomalies Detected: {len(data[\\'anomalies\\'])}\")\\n        print(\\'\\')', approval='true'), CodeApprovalItem(code='def create_correlation_matrix(file_path):\\n    import pandas as pd\\n    import seaborn as sns\\n    import matplotlib.pyplot as plt\\n\\n    # Load data from the given file path\\n    df = pd.read_csv(file_path)\\n\\n    # Define the columns of interest\\n    columns_of_interest = [\\'CPU\\', \\'USED_GB\\', \\'SWAP_MB\\', \\'BUSY_SQL\\']\\n\\n    # Check that all required columns are present\\n    missing_columns = [col for col in columns_of_interest if col not in df.columns]\\n    if missing_columns:\\n        raise ValueError(f\"Missing columns in data file: {missing_columns}\")\\n\\n    # Calculate the correlation matrix of the selected columns\\n    corr_matrix = df[columns_of_interest].corr()\\n\\n    # Create a heatmap for the correlation matrix\\n    plt.figure(figsize=(8, 6))\\n    sns.heatmap(corr_matrix, annot=True, cmap=\\'coolwarm\\')\\n    plt.title(\\'Correlation Matrix of System Metrics\\')\\n    plt.xlabel(\\'Metrics\\')\\n    plt.ylabel(\\'Metrics\\')\\n    plt.tight_layout()\\n    plt.show()\\n\\n# Example usage:\\n# create_correlation_matrix(\\'your_file.csv\\')', approval='true'), CodeApprovalItem(code='import pandas as pd\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.preprocessing import StandardScaler\\nimport matplotlib.pyplot as plt\\n\\n\\ndef cluster_system_performance(file_path):\\n    \"\"\"\\n    Reads system performance data from a CSV file, performs clustering to categorize snapshots into\\n    different operational states and saves the resulting dataframe with cluster labels to a new CSV file.\\n    \\n    Parameters:\\n    file_path (str): Path to the input CSV file containing system performance data.\\n    \\n    Returns:\\n    DataFrame: The original dataframe augmented with a \\'Performance_Profile\\' column indicating cluster labels.\\n    \"\"\"\\n    # Load data from the provided CSV file\\n    df = pd.read_csv(file_path)\\n\\n    # Select features for clustering\\n    features = [\\'PING_MS\\', \\'CPU\\', \\'USED_GB\\', \\'SWAP_MB\\', \\'CONNS\\', \\'TRANS\\', \\'WAIT_THR\\', \\'BUSY_SQL\\', \\'PEND_SESS\\', \\'D_WR_MBPS\\', \\'DOWN_EVENT\\']\\n    X = df[features].fillna(0)  # Handle missing values if any\\n\\n    # Standardize features\\n    scaler = StandardScaler()\\n    X_scaled = scaler.fit_transform(X)\\n\\n    # Determine optimal number of clusters using the elbow method\\n    wcss = []\\n    for i in range(1, 11):\\n        kmeans = KMeans(n_clusters=i, random_state=42)\\n        kmeans.fit(X_scaled)\\n        wcss.append(kmeans.inertia_)\\n\\n    # Optional: Plot elbow curve to choose the number of clusters\\n    plt.figure()\\n    plt.plot(range(1, 11), wcss, marker=\\'o\\')\\n    plt.xlabel(\\'Number of clusters\\')\\n    plt.ylabel(\\'Within-cluster Sum of Squares (WCSS)\\')\\n    plt.title(\\'Elbow Method for Optimal k\\')\\n    plt.show()\\n\\n    # From the elbow plot, choose the optimal number of clusters, e.g., 3\\n    optimal_k = 3\\n    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\\n    clusters = kmeans.fit_predict(X_scaled)\\n\\n    # Add cluster labels to the original dataframe\\n    df[\\'Performance_Profile\\'] = clusters\\n\\n    # Save the result to a new CSV file\\n    output_file = \\'system_performance_clusters.csv\\'\\n    df.to_csv(output_file, index=False)\\n    print(f\\'Results saved to {output_file}\\')\\n\\n    return df\\n\\n\\n# Example usage:\\n# df_result = cluster_system_performance(\\'system_performance_data.csv\\')\\n', approval='true')], CodeApprovalItem(code='import pandas as pd\\nimport matplotlib.pyplot as plt\\nimport matplotlib.dates as mdates\\n\\n\\ndef analyze_network_performance(file_path):\\n    \"\"\"\\n    Analyzes network performance metrics from a CSV file and generates time series plots to show raw,\\n    smoothed data (via moving average), and anomalies based on 3 standard deviations for PING_MS and CONNS.\\n\\n    The CSV file is expected to have a header containing at least the following columns:\\n    - SNAPSHOT_TIME (dates, will be parsed)\\n    - PING_MS\\n    - CONNS\\n    \\n    Parameters:\\n    file_path (str): The path to the CSV data file.\\n    \"\"\"\\n    # Read the data assuming CSV format with header row and parse \\'SNAPSHOT_TIME\\' column as datetime\\n    df = pd.read_csv(file_path, parse_dates=[\\'SNAPSHOT_TIME\\'])\\n    \\n    # Set SNAPSHOT_TIME as the index for time series analysis\\n    df.set_index(\\'SNAPSHOT_TIME\\', inplace=True)\\n    \\n    # Plot PING_MS over time\\n    plt.figure(figsize=(14, 6))\\n    plt.plot(df.index, df[\\'PING_MS\\'], label=\\'PING_MS\\')\\n    plt.title(\\'Network PING_MS Over Time\\')\\n    plt.xlabel(\\'Time\\')\\n    plt.ylabel(\\'PING_MS (ms)\\')\\n    plt.legend()\\n    plt.grid(True)\\n    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\\'%Y-%m-%d %H:%M\\'))\\n    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\\n    plt.gcf().autofmt_xdate()\\n    plt.show()\\n    \\n    # Plot CONNS over time\\n    plt.figure(figsize=(14, 6))\\n    plt.plot(df.index, df[\\'CONNS\\'], label=\\'CONNS\\')\\n    plt.title(\\'Network CONNS Over Time\\')\\n    plt.xlabel(\\'Time\\')\\n    plt.ylabel(\\'Number of Connections\\')\\n    plt.legend()\\n    plt.grid(True)\\n    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\\'%Y-%m-%d %H:%M\\'))\\n    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\\n    plt.gcf().autofmt_xdate()\\n    plt.show()\\n    \\n    # Calculate moving averages (using a window of 10 data points)\\n    df[\\'PING_MS_MA\\'] = df[\\'PING_MS\\'].rolling(window=10).mean()\\n    df[\\'CONNS_MA\\'] = df[\\'CONNS\\'].rolling(window=10).mean()\\n    \\n    # Plot smoothed PING_MS with moving average\\n    plt.figure(figsize=(14, 6))\\n    plt.plot(df.index, df[\\'PING_MS\\'], alpha=0.3, label=\\'PING_MS Raw\\')\\n    plt.plot(df.index, df[\\'PING_MS_MA\\'], label=\\'PING_MS Moving Avg\\', linewidth=2)\\n    plt.title(\\'Smoothed Network PING_MS\\')\\n    plt.xlabel(\\'Time\\')\\n    plt.ylabel(\\'PING_MS (ms)\\')\\n    plt.legend()\\n    plt.grid(True)\\n    plt.show()\\n    \\n    # Plot smoothed CONNS with moving average\\n    plt.figure(figsize=(14, 6))\\n    plt.plot(df.index, df[\\'CONNS\\'], alpha=0.3, label=\\'CONNS Raw\\')\\n    plt.plot(df.index, df[\\'CONNS_MA\\'], label=\\'CONNS Moving Avg\\', linewidth=2)\\n    plt.title(\\'Smoothed Network CONNS\\')\\n    plt.xlabel(\\'Time\\')\\n    plt.ylabel(\\'Number of Connections\\')\\n    plt.legend()\\n    plt.grid(True)\\n    plt.show()\\n    \\n    # Detect anomalies using a threshold of 3 standard deviations from the mean\\n    ping_mean = df[\\'PING_MS\\'].mean()\\n    ping_std = df[\\'PING_MS\\'].std()\\n    conns_mean = df[\\'CONNS\\'].mean()\\n    conns_std = df[\\'CONNS\\'].std()\\n    \\n    df[\\'PING_MS_anomaly\\'] = (df[\\'PING_MS\\'] > ping_mean + 3 * ping_std) | (df[\\'PING_MS\\'] < ping_mean - 3 * ping_std)\\n    df[\\'CONNS_anomaly\\'] = (df[\\'CONNS\\'] > conns_mean + 3 * conns_std) | (df[\\'CONNS\\'] < conns_mean - 3 * conns_std)\\n    \\n    # Plot anomalies detected in PING_MS\\n    plt.figure(figsize=(14, 6))\\n    colors = df[\\'PING_MS_anomaly\\'].apply(lambda x: \\'red\\' if x else \\'blue\\')\\n    plt.scatter(df.index, df[\\'PING_MS\\'], c=colors, label=\\'PING_MS Anomaly Points\\')\\n    plt.title(\\'PING_MS with Anomalies\\')\\n    plt.xlabel(\\'Time\\')\\n    plt.ylabel(\\'PING_MS (ms)\\')\\n    plt.legend()\\n    plt.grid(True)\\n    plt.show()\\n    \\n    # Plot anomalies detected in CONNS\\n    plt.figure(figsize=(14, 6))\\n    colors = df[\\'CONNS_anomaly\\'].apply(lambda x: \\'red\\' if x else \\'blue\\')\\n    plt.scatter(df.index, df[\\'CONNS\\'], c=colors, label=\\'CONNS Anomaly Points\\')\\n    plt.title(\\'CONNS with Anomalies\\')\\n    plt.xlabel(\\'Time\\')\\n    plt.ylabel(\\'Number of Connections\\')\\n    plt.legend()\\n    plt.grid(True)\\n    plt.show()\\n\\n\\n# Example usage:\\n# analyze_network_performance(\\'path_to_your_data.csv\\')\\n\\n\\nif __name__ == \\'__main__\\':\\n    # Example: Replace \\'data.csv\\' with your file path to run the analysis.\\n    file_path = \\'data.csv\\'\\n    analyze_network_performance(file_path)', approval='true'), CodeApprovalItem(code='import pandas as pd\\n\\ndef create_feature_engineering_pipeline(file_path):\\n    \"\"\"Load data from a CSV file and compute aggregated statistics (mean, median, max, min) for specified numerical columns grouped by \\'HOST\\' and \\'PORT\\'.\"\"\"\\n    # Load the dataset from the file\\n    df = pd.read_csv(file_path)\\n    \\n    # Specify the numerical columns to analyze\\n    numerical_cols = [\\'PING_MS\\', \\'CPU\\', \\'USED_GB\\', \\'SWAP_MB\\', \\'CONNS\\', \\'TRANS\\', \\'WAIT_THR\\', \\'BUSY_SQL\\', \\'PEND_SESS\\', \\'D_WR_MBPS\\', \\'DOWN_EVENT\\']\\n    \\n    # Group by \\'HOST\\' and \\'PORT\\'\\n    grouped = df.groupby([\\'HOST\\', \\'PORT\\'])\\n    \\n    # Calculate aggregated statistics\\n    aggregated = grouped[numerical_cols].agg([\\'mean\\', \\'median\\', \\'max\\', \\'min\\'])\\n    \\n    # Flatten multi-level columns\\n    aggregated.columns = [\\'_\\'.join(col).strip() for col in aggregated.columns.values]\\n    \\n    # Reset index to turn grouping columns back into columns\\n    summary_df = aggregated.reset_index()\\n    \\n    return summary_df\\n\\n# Example usage:\\n# summary = create_feature_engineering_pipeline(\\'your_data.csv\\')\\n# print(summary)\\n', approval='true'), CodeApprovalItem(code='import pandas as pd\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.preprocessing import StandardScaler\\n\\n\\ndef cluster_hosts(file_path):\\n    \"\"\"\\n    Load host resource utilization and network activity data from a CSV file, \\n    standardize the features, and cluster the hosts using KMeans.\\n    \\n    Args:\\n        file_path (str): Path to the CSV data file.\\n    \\n    Returns:\\n        DataFrame: The original DataFrame with an additional \\'Cluster\\' column representing the cluster assignment.\\n    \"\"\"\\n    # Load data from the file\\n    df = pd.read_csv(file_path, parse_dates=[\\'SNAPSHOT_TIME\\'])\\n\\n    # Select features for clustering (excluding non-numeric or identifier columns)\\n    features = [\\'CPU\\', \\'USED_GB\\', \\'SWAP_MB\\', \\'CONNS\\', \\'TRANS\\', \\'WAIT_THR\\']\\n    X = df[features].fillna(0)  # Fill NaN values if any\\n\\n    # Standardize features\\n    scaler = StandardScaler()\\n    X_scaled = scaler.fit_transform(X)\\n\\n    # Determine the optimal number of clusters using the elbow method (here simplified by assuming 3 clusters)\\n    # In practice, you might want to analyze the distortions list to choose k.\\n    n_clusters = 3\\n\\n    # Fit final model using KMeans\\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\\n    clusters = kmeans.fit_predict(X_scaled)\\n\\n    # Add cluster labels to the original DataFrame\\n    df[\\'Cluster\\'] = clusters\\n    \\n    return df\\n\\n\\nif __name__ == \\'__main__\\':\\n    # Example usage\\n    file_path = \\'your_data_file.csv\\'  # Replace with your actual file path\\n    clustered_df = cluster_hosts(file_path)\\n    print(clustered_df.head())\\n', approval='true'), [CodeApprovalItem(code='import pandas as pd\\nimport matplotlib.pyplot as plt\\nimport matplotlib.dates as mdates\\n\\n\\ndef analyze_network_performance(file_path):\\n    \"\"\"\\n    Analyzes network performance metrics from a CSV file and generates time series plots to show raw,\\n    smoothed data (via moving average), and anomalies based on 3 standard deviations for PING_MS and CONNS.\\n\\n    The CSV file is expected to have a header containing at least the following columns:\\n    - SNAPSHOT_TIME (dates, will be parsed)\\n    - PING_MS\\n    - CONNS\\n    \\n    Parameters:\\n    file_path (str): The path to the CSV data file.\\n    \"\"\"\\n    # Read the data assuming CSV format with header row and parse \\'SNAPSHOT_TIME\\' column as datetime\\n    df = pd.read_csv(file_path, parse_dates=[\\'SNAPSHOT_TIME\\'])\\n    \\n    # Set SNAPSHOT_TIME as the index for time series analysis\\n    df.set_index(\\'SNAPSHOT_TIME\\', inplace=True)\\n    \\n    # Plot PING_MS over time\\n    plt.figure(figsize=(14, 6))\\n    plt.plot(df.index, df[\\'PING_MS\\'], label=\\'PING_MS\\')\\n    plt.title(\\'Network PING_MS Over Time\\')\\n    plt.xlabel(\\'Time\\')\\n    plt.ylabel(\\'PING_MS (ms)\\')\\n    plt.legend()\\n    plt.grid(True)\\n    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\\'%Y-%m-%d %H:%M\\'))\\n    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\\n    plt.gcf().autofmt_xdate()\\n    plt.show()\\n    \\n    # Plot CONNS over time\\n    plt.figure(figsize=(14, 6))\\n    plt.plot(df.index, df[\\'CONNS\\'], label=\\'CONNS\\')\\n    plt.title(\\'Network CONNS Over Time\\')\\n    plt.xlabel(\\'Time\\')\\n    plt.ylabel(\\'Number of Connections\\')\\n    plt.legend()\\n    plt.grid(True)\\n    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\\'%Y-%m-%d %H:%M\\'))\\n    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\\n    plt.gcf().autofmt_xdate()\\n    plt.show()\\n    \\n    # Calculate moving averages (using a window of 10 data points)\\n    df[\\'PING_MS_MA\\'] = df[\\'PING_MS\\'].rolling(window=10).mean()\\n    df[\\'CONNS_MA\\'] = df[\\'CONNS\\'].rolling(window=10).mean()\\n    \\n    # Plot smoothed PING_MS with moving average\\n    plt.figure(figsize=(14, 6))\\n    plt.plot(df.index, df[\\'PING_MS\\'], alpha=0.3, label=\\'PING_MS Raw\\')\\n    plt.plot(df.index, df[\\'PING_MS_MA\\'], label=\\'PING_MS Moving Avg\\', linewidth=2)\\n    plt.title(\\'Smoothed Network PING_MS\\')\\n    plt.xlabel(\\'Time\\')\\n    plt.ylabel(\\'PING_MS (ms)\\')\\n    plt.legend()\\n    plt.grid(True)\\n    plt.show()\\n    \\n    # Plot smoothed CONNS with moving average\\n    plt.figure(figsize=(14, 6))\\n    plt.plot(df.index, df[\\'CONNS\\'], alpha=0.3, label=\\'CONNS Raw\\')\\n    plt.plot(df.index, df[\\'CONNS_MA\\'], label=\\'CONNS Moving Avg\\', linewidth=2)\\n    plt.title(\\'Smoothed Network CONNS\\')\\n    plt.xlabel(\\'Time\\')\\n    plt.ylabel(\\'Number of Connections\\')\\n    plt.legend()\\n    plt.grid(True)\\n    plt.show()\\n    \\n    # Detect anomalies using a threshold of 3 standard deviations from the mean\\n    ping_mean = df[\\'PING_MS\\'].mean()\\n    ping_std = df[\\'PING_MS\\'].std()\\n    conns_mean = df[\\'CONNS\\'].mean()\\n    conns_std = df[\\'CONNS\\'].std()\\n    \\n    df[\\'PING_MS_anomaly\\'] = (df[\\'PING_MS\\'] > ping_mean + 3 * ping_std) | (df[\\'PING_MS\\'] < ping_mean - 3 * ping_std)\\n    df[\\'CONNS_anomaly\\'] = (df[\\'CONNS\\'] > conns_mean + 3 * conns_std) | (df[\\'CONNS\\'] < conns_mean - 3 * conns_std)\\n    \\n    # Plot anomalies detected in PING_MS\\n    plt.figure(figsize=(14, 6))\\n    colors = df[\\'PING_MS_anomaly\\'].apply(lambda x: \\'red\\' if x else \\'blue\\')\\n    plt.scatter(df.index, df[\\'PING_MS\\'], c=colors, label=\\'PING_MS Anomaly Points\\')\\n    plt.title(\\'PING_MS with Anomalies\\')\\n    plt.xlabel(\\'Time\\')\\n    plt.ylabel(\\'PING_MS (ms)\\')\\n    plt.legend()\\n    plt.grid(True)\\n    plt.show()\\n    \\n    # Plot anomalies detected in CONNS\\n    plt.figure(figsize=(14, 6))\\n    colors = df[\\'CONNS_anomaly\\'].apply(lambda x: \\'red\\' if x else \\'blue\\')\\n    plt.scatter(df.index, df[\\'CONNS\\'], c=colors, label=\\'CONNS Anomaly Points\\')\\n    plt.title(\\'CONNS with Anomalies\\')\\n    plt.xlabel(\\'Time\\')\\n    plt.ylabel(\\'Number of Connections\\')\\n    plt.legend()\\n    plt.grid(True)\\n    plt.show()\\n\\n\\n# Example usage:\\n# analyze_network_performance(\\'path_to_your_data.csv\\')\\n\\n\\nif __name__ == \\'__main__\\':\\n    # Example: Replace \\'data.csv\\' with your file path to run the analysis.\\n    file_path = \\'data.csv\\'\\n    analyze_network_performance(file_path)', approval='true'), CodeApprovalItem(code='import pandas as pd\\n\\ndef create_feature_engineering_pipeline(file_path):\\n    \"\"\"Load data from a CSV file and compute aggregated statistics (mean, median, max, min) for specified numerical columns grouped by \\'HOST\\' and \\'PORT\\'.\"\"\"\\n    # Load the dataset from the file\\n    df = pd.read_csv(file_path)\\n    \\n    # Specify the numerical columns to analyze\\n    numerical_cols = [\\'PING_MS\\', \\'CPU\\', \\'USED_GB\\', \\'SWAP_MB\\', \\'CONNS\\', \\'TRANS\\', \\'WAIT_THR\\', \\'BUSY_SQL\\', \\'PEND_SESS\\', \\'D_WR_MBPS\\', \\'DOWN_EVENT\\']\\n    \\n    # Group by \\'HOST\\' and \\'PORT\\'\\n    grouped = df.groupby([\\'HOST\\', \\'PORT\\'])\\n    \\n    # Calculate aggregated statistics\\n    aggregated = grouped[numerical_cols].agg([\\'mean\\', \\'median\\', \\'max\\', \\'min\\'])\\n    \\n    # Flatten multi-level columns\\n    aggregated.columns = [\\'_\\'.join(col).strip() for col in aggregated.columns.values]\\n    \\n    # Reset index to turn grouping columns back into columns\\n    summary_df = aggregated.reset_index()\\n    \\n    return summary_df\\n\\n# Example usage:\\n# summary = create_feature_engineering_pipeline(\\'your_data.csv\\')\\n# print(summary)\\n', approval='true'), CodeApprovalItem(code='import pandas as pd\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.preprocessing import StandardScaler\\n\\n\\ndef cluster_hosts(file_path):\\n    \"\"\"\\n    Load host resource utilization and network activity data from a CSV file, \\n    standardize the features, and cluster the hosts using KMeans.\\n    \\n    Args:\\n        file_path (str): Path to the CSV data file.\\n    \\n    Returns:\\n        DataFrame: The original DataFrame with an additional \\'Cluster\\' column representing the cluster assignment.\\n    \"\"\"\\n    # Load data from the file\\n    df = pd.read_csv(file_path, parse_dates=[\\'SNAPSHOT_TIME\\'])\\n\\n    # Select features for clustering (excluding non-numeric or identifier columns)\\n    features = [\\'CPU\\', \\'USED_GB\\', \\'SWAP_MB\\', \\'CONNS\\', \\'TRANS\\', \\'WAIT_THR\\']\\n    X = df[features].fillna(0)  # Fill NaN values if any\\n\\n    # Standardize features\\n    scaler = StandardScaler()\\n    X_scaled = scaler.fit_transform(X)\\n\\n    # Determine the optimal number of clusters using the elbow method (here simplified by assuming 3 clusters)\\n    # In practice, you might want to analyze the distortions list to choose k.\\n    n_clusters = 3\\n\\n    # Fit final model using KMeans\\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\\n    clusters = kmeans.fit_predict(X_scaled)\\n\\n    # Add cluster labels to the original DataFrame\\n    df[\\'Cluster\\'] = clusters\\n    \\n    return df\\n\\n\\nif __name__ == \\'__main__\\':\\n    # Example usage\\n    file_path = \\'your_data_file.csv\\'  # Replace with your actual file path\\n    clustered_df = cluster_hosts(file_path)\\n    print(clustered_df.head())\\n', approval='true')]]}}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T10:02:15.456106Z",
     "start_time": "2025-05-17T10:02:15.451999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print(f'=======SPACE_HOLDER========')\n",
    "# print(s_lists[-1]['space_holder'])\n",
    "# print(f'=======orig_file_path========')\n",
    "# print(s_lists[-1]['space_holder']['orig_file_path'])\n",
    "print(f'=======code_approval_items========')\n",
    "# print(s_lists[-1]['space_holder']['code_approval_items'])\n",
    "code_approval_items_list = s_lists[-1]['space_holder']['code_approval_items']\n",
    "for item in code_approval_items_list:\n",
    "    print(\"================================\")\n",
    "    if not isinstance(item, list):\n",
    "        i = item.model_dump()\n",
    "        print(type(i))\n",
    "        print(i.keys())\n",
    "        print(type(list(i.keys())))\n",
    "    else:\n",
    "        i = item[0]\n",
    "        i = i.model_dump()\n",
    "        print(type(i))\n",
    "\n",
    "    # print(item)"
   ],
   "id": "64c6d2e22d5401e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======code_approval_items========\n",
      "================================\n",
      "<class 'dict'>\n",
      "dict_keys(['code', 'approval'])\n",
      "<class 'list'>\n",
      "================================\n",
      "<class 'dict'>\n",
      "dict_keys(['code', 'approval'])\n",
      "<class 'list'>\n",
      "================================\n",
      "<class 'dict'>\n",
      "dict_keys(['code', 'approval'])\n",
      "<class 'list'>\n",
      "================================\n",
      "<class 'dict'>\n",
      "================================\n",
      "<class 'dict'>\n",
      "dict_keys(['code', 'approval'])\n",
      "<class 'list'>\n",
      "================================\n",
      "<class 'dict'>\n",
      "dict_keys(['code', 'approval'])\n",
      "<class 'list'>\n",
      "================================\n",
      "<class 'dict'>\n",
      "dict_keys(['code', 'approval'])\n",
      "<class 'list'>\n",
      "================================\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
